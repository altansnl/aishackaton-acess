Introduction Since the beginning of the modern social sciences, comparisons between countries, cultures and communities  were  their  constitutive  part.  Large  portions  of  such  comparisons  are  focused  on constructs  such  as  personality  traits,  attitudes,  values,  worldviews  and  norms  that  usually  are measured  by  multiple  indicators.  Such  constructs  are  usually  considered  as  continuous  latent variables and the observed responses to questions are treated as manifestations of those latent traits (Brown, 2015; Joreskog, 1973). For example, the level of religiosity is usually measured by several questions on general religiousness and religious practices (e.g. Lemos, Gore, Puga-Gonzalez, & Shults, 2019), attitudes to migrants are sometimes inferred by several items describing willingness to allow immigrants into the country (e.g. Davidov, Meuleman, Billiet & Schmidt, 2008), while the level of political participation is routinely identified by asking whether respondents had taken part in various forms of political activities (e.g. Kostelka 2014).  To assure the observed differences between groups are reflecting the true differences between latent constructs and are not biased by artifacts induced by differences in measurement tools, the assumption of measurement invariance (or measurement equivalence) needs to be fulfilled. This is a property of a measurement, stating that the instrument measures the same constructs in the same way across various subgroups of respondents (Davidov et al. 2008). Mellenbergh (1989) pointed  out  that  measurement  invariance  could  be  expressed  as  assumption  of  conditional independence:  f given the latent trait ğœƒ is conditionally independent of group membership G for given group g. In Measurement invariance holds when the probability of given response U using response function the  context  of  cross-country  comparisons  violation  of  this  assumption  is  usually  called measurement non-invariance, while in psychometrics the term Differential Item Functioning (DIF) is more common (Holland & Wainer, 1993; Millsap, 2011).  ğ‘“(ğ‘ˆ|ğœƒ,ğº=ğ‘”)=ğ‘“(ğ‘ˆ|ğœƒ)       (1) Different nomenclature goes along with different problems and different methods of detecting violations of conditional independence. DIF methods were designed to cope with cognitive testing and psychological assessments where number of groups is limited to two in the majority of the cases (usually gender or minority status), number of items is large, and violations of conditional independence are rather rare, indicating some serious flaws in particular items. The research on DIF is rich and resulted in series of well-established and well-evaluated methods (Angoff 1972, Lord 1980, Swaminathan & Rogers, 1990, Swaminathan & Rogers, 1990, Thissen, Steinberg, & Wainer 1993, Penfield, & Lam, 2000, Holland & Wainer 2012, Kopf, Zeileis, & Strobl 2015).  In  cross-countries  studies  settings  are  very  different,  since  the  number  of  groups  is  usually significantly larger than two. For example, the European Social Survey (ESS) surveys samples from around 20 to 30 countries, depending on a round, while the newest 7th wave of the World Values Survey has conducted representative national surveys in as much as 80 countries. In 1984, the International Social Survey (ISSP) started with 6 countries and has grown since to include 40 nations in 2020. The record in the number of participating countries belongs to the Gallup Global Well-Being  Survey  that  in  2019  collected  data  from  144  countries,  and  these  are  just  a  few examples.  DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 3 As large comparative surveys aim to ask questions concerning multiple topics, usually the length of instruments used to measure individual constructs need to be short. For example, in round 9 of ESS (European Social Survey 2018) the average number of items per scale was just 4.6 while in the Survey of Adult Skills (OECD 2010) it was equal to 3.8. Moreover, in the context of cross-cultural studies, violation of measurement invariance (or DIF) is more of a rule than an exception (Davidov, Meuleman, Billiet & Schmidt 2008, Pokropek and Borgonovi 2017). In cross-cultural studies respondents come from different countries, usually speak different languages, have been socialized  in  diverse  cultural  backgrounds  and  might  understand  certain  ideas  or  concepts  in different,  culturally-varying  manners.  Furthermore,  the  concepts  measured  like  trust,  values, attitudes, or political opinions are much more ambiguous and context dependent than cognitive skills or personality traits.  Although  in  the  context  of  cross-cultural  studies  various  methods  were  developed  to  assess measurement  invariance  of  an  instrument  as  a  whole,  that  is  checking  whether  measurement invariance simultaneously holds for all items in the scale (Kim, Cao, Wang, & Nguyen 2017), methods designed to point out specific item or item parameter stayed underrepresented, being not very efficient (Finch, 2016; Kim et al., 1995; Magis et al., 2011; Penfield, 2001) and rarely tested (Davidov  et  al.  2014,  Fitzgerald  &  Jowell  2010,  Skjak  2010,  van  de  Vijver  2011).  This  is  a frustrating gap especially in the light of the fact that well-performing models were developed to compensate  for  the  non-invariance  item  parameters  but  only  if  non-invariant  parameters  are correctly  identified  (Byrne,  Shavelson  &  MuthÃ©n  1989;  Steinmetz  2013,  van  de  Schoot, Kluytmans, Tummers, Lugtig, Hox, & MuthÃ©n 2013, Asparouhov & MuthÃ©n 2014; De Jong et al. 2007,  MuthÃ©n  &  Asparouhov,  2012,  2014).  In  other  words,  we  have  tools  to  fix  some  of  the problems of comparability but we do not know where to apply them. To solve this problem, we are applying methodology based on Deep Neural Networks (DNN), an approach that has shown notable success in anomaly detection problems in industrial (e.g. Staar, LÃ¼tjen & Freitag 2019), medical (e.g. Anwar, Majid, Qayyum, Awais, Alnowami, & Khan, 2018) or cyber security (Ferrag, Maglaras, Moschoyiannis & Janicke, 2020) settings. We propose to look at the problem of measurement non-invariance of items or DIF as an anomaly detection task that could be handled by DNN combined with simulation approach. The idea is to train DNN in a controlled environment provided by simulation data and teach it to detect non-invariance instances in real scenarios. In fact, from a statistical point of view there are no contraindications to treat non-invariant items in cross-country studies similarly to broken screws in mechanical devices, brain tumors in magnetic resonance imaging scans or wire transfer frauds in finances.  The  contribution  of  this  paper  is  threefold.  First,  we  review  and  discuss  existing  methods  for detecting non-invariance item parameters (or DIFs) that could be used in settings of cross-country comparability  group  comparisons.  Second,  we  introduce  a  new  method  for  detecting  non-invariance in item parameters, test it in a simulation study and present an application on a real data example. Third, we confront newly proposed approaches with the existing ones, contributing to the ongoing research by validating different methods under various new conditions.    DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 4 1.1 Existing methods for detecting non-invariant items in multiple-group settings There are several methods for detecting non-invariance, however not all of them are suitable for multiple-groups settings. For example, the most popular methods based on generalized logistic regression approach (Magis et al., 2011) are designed to detect non-invariant items, but without specifying in which group the invariance holds, and in which it does not, just flagging the particular items  as  non-invariant  between  all  of  the  groups.  Some  of  the  methods  like  Mantelâ€“Haenszel procedures (Penfield, 2001b) are not able to detect invariance in item loadings (called also non-uniform  Differential  Item  Functioning;  DIF).  Most  of  the  methods,  even  those  with  the  word â€œmultiple-groupsâ€ in its name, were designed and tested for 3 or 4 groups (Fidalgo & Scanlon 2010, Woods, Cai, & Wang 2013). Other methods like random item effects models (De Boeck 2008; Fox, 2010) require a very large number of groups (more than 40 according to some accounts (Meuleman & Billiet, 2009)) that could be suitable only for the largest comparative programs. For this article we have chosen methods governed by the three criteria: (1) method could be applied to moderate and large numbers of groups, (2) provides information of non-invariance for both item parameters  (thresholds  and  loadings),  or  in  other  words,  could  detect  both  uniform  and  non-uniform DIF and (3) provides information on item non-invariance for each group. We have chosen two approaches from latent variable framework: CFA/IRT model misfit indices, sequential item parameters  comparison  implemented  in  the  alignment  method,  and  one  approach  based  on observed scores: pairwise logistic regression DIF detection. 1.2 Methods of detecting non-invariance items in latent variable framework Most popular approach to address the problems of group comparisons and potential compatibility problems in cross-country settings is the latent variable framework (Davidov et al. 2008). In this paper  we  will  restrict  our  investigation  to  the  case  where  indicators  are  binary,  and  therefore multiple group Item Response Theory (IRT) models will be used (Lord, 1980; W. J. van der Linden & Hambleton, 2013). However, the approaches we discuss here are easily applicable to continuous (Brown, 2015; Joreskog, 1973) and polytomous indicators (W. J. van der Linden & Hambleton, 2013). Some authors refer to IRT models as CFA models with categorical indicators. In fact, IRT and CFA models for categorical data are essentially the same models with some small differences in parametrization. In this paper we are treating IRT and CFA models for categorical data as the same models. Let ğ‘ˆ!"#	be a dichotomous indicator (response to the item) for a latent factor ğœƒ"#. According to the IRT modelling, the probability of responding positively on item i (ğ‘ˆ!"#=1) is given by the logistic model: ğ‘ƒ!"#(ğ‘ˆ!"#|ğœƒ"#)=	ğ‘ƒ!"#.ğœƒ"#,ğº=ğ‘”/=where i = 1,â€¦,I denotes the item index, p the person index and g = 1,â€¦,NG the group index, , ğœ!# and ğœ†!#  are  the  item  parameters,  threshold  (in  some  contexts  named  intercepts),  and  loading respectively, ğœƒ"# is a factor reflecting the latent trait that is assumed to be normally distributed in (1) $%&â€™"	(*!"+,!"-#")	,ğœƒ"#âˆ¼ğ‘(ğ›¼#,ğœ“#/)	  $each group. The graphical representation for four groups is presented in Figure 1.  DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 5   Figure 1. Graphical representation of four group IRT model  group means ğ›¼=(ğ›¼#) and standard deviations. The standard approach in multiple-group settings Unless some restrictions on parameters are imposed, the multiple group IRT model is not identified because not all item loadings and item thresholds can be simultaneously estimated along with is to constrain all item parameters to be the same in all groups and fixing the mean together with variance  of  one  of  the  groups  to  0  and  1  respectively.  Alternative  identification  strategies  are possible  but  this  is  the  most  common  in  multiple-group  settings  (see  Svetina,  Rutkowski  & Rutkowski, 2020).  This model is often referred to as a scalar model and assumes full-invariance of item parameters (both loadings and thresholds).  Assuming conditional independence of the responses, the models could be estimated using various techniques including maximum marginal likelihood (Bock & Zimowski, 1997), weighted least squares estimations and its extensions (Beauducel & Herzberg, 2006; Li, 2016) as well as Bayesian estimation (Fox, 2010).  Item fit statistics of scalar model. The popular approach for detecting item non-invariance is based on assessing the so-called modification indices criteria (MI, also referred to as the univariate Lagrange  multiplier  (Sarris,  Satorra  &  Sorbom  1987,  Sorbom,  1989).  The  modification  index expresses an approximation of how much the overall model ğœ’2 would change when the parameter is freed from a constraint. In the context of a multi-group model this means that item parameter is estimated separately instead of being constrained to be equal in all groups. Next statistic, expected parameter change (EPC) index is strictly related to MI. EPC signals the value of the expected parameter change between the fixed and freely estimated parameter. In the context of measurement invariance EPC expresses the difference between fixed item parameter estimation across groups and free estimation of this parameter for a certain group (Kaplan 1989; Whittaker, 2012). The EPC is  in  fact  directly  analogous  to  MI  but  reflecting  the  approximate  changes  in  metric  of  the parameters  which  might  help  experienced  scientists  to  interpret  it  better  and  formulate  more meaningful cut-off criteria. Saris et al. (2009) noticed, the EPCs are consistent estimates of the true value of the parameter, and MI gives good indication of local misfit, provided that the other DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 6 restrictions in the model are (approximately) correct, which was confirmed in simulation studies (MacCallum 1986, MacCallum & Austin 2000).  Described fit statistics have similar problems. First, no established thresholds for those measures exist and in operational use researchers rely on subjective decisions. Second, the consistency of the indices, that is their increasing ability to produce correct inferences with the increased sample size depends on the overall fit of the model to the data. Third, those fit statistics work well for large non-invariance violations but when differences between latent means of the groups are not large the power of DIF detection decreases significantly. This what was previously noticed by Oort (1998) in a simulation study performed using two group scenarios investigating MI and EPC and was  later  confirmed  in  multiple-group  settings  and  for  other  similar  approaches  (Buchholz  & Hartig 2020, Tijmstra, Bolsinova, Liaw, Rutkowski & Rutkowski 2020).  Alignment sequential DIF algorithm was proposed by Asparouhov and MuthÃ©n (2014) and then further developed by other researchers (Robitzsch 2020; Pokropek, LÃ¼dtke, and Robitzsch 2020). It  aligns  item  parameters  from  group-specific  configural  CFA  or  IRT  models  (where  item parameters  are  estimated  without  equality  constraints  across  groups)  into  the  most  optimal invariance pattern that allows the estimation of group-specific factor means and variances without requiring the exact measurement invariance. It is doing that by determining Î± and Ïˆ in such a way that  the  amount  of  measurement  non-invariance  is  minimized.  This  is  done  by  an  alignment function that optimally aligns group-specific item parameters and such procedure consists of two steps.  ğœ†!#=,!",%0"  and   ğœ!#=ğœ!#,2âˆ’,!",%0"ğ›¼# , First, configural measurement models are estimated for each group, where each model is identified by  setting  the  mean  to  zero  and  the  standard  deviations  to  one  while  all  item  parameters  are estimated  freely  in  each  of  the  groups.  Because  the  means  and  standard  deviations  of  latent variables are functions of item parameters: where ğœ†!#,2 and ğœ!#,2 are loadings and thresholds from a configural model respectively, shifting (3)	where x is the difference between item parameters and ğœ– is a small number such as 0.0001 to make values of means and standard deviations of group would result in changes of item parameters. This relation  is  used  in  the  second  step  of  the  alignment  algorithm  where  procedure  searches  for configuration of means and standard deviations that would minimize the difference between item parameters between groups. To achieve this MuthÃ©n and Asparouhov (2014) proposed to use a loss function that would be the same for loadings and thresholds:  ğ‘“=âˆšğ‘¥/+ğœ–&			the function differentiable. Alignment procedure penalizes differences in item thresholds and item loadings  between  groups  and  hence  minimizes  the  extent  of  measurement  non-invariance according to the loss function. There are various possibilities of specifying implementation of the alignment optimization. In our application we used fixed version with the default settings of Mplus (MuthÃ©n and Asparouhov 2014)   (2)  	 		DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 7 For  detecting  non-invariant  item  parameters  an  iterative  pairwise  procedure  is  used  after  the alignment  estimation  is  completed.  This  procedure  is  conducted  for  each  item  parameter separately. In the first step for each item parameter the largest invariant set of groups is determined. The group is found by conducting pairwise tests for each pair of groups on each parameter. If the differences  between  item  parameters  are  not  significant  (p>0.001)  pairs  of  parameters  are connected. The largest connected set of groups is considered as the starting invariant set. In the second step the average of each parameter from the starting invariant set is computed. Then the parameter from each group is compared to the average of the parameters in the invariant set. If there is a significant difference, the group will be removed from the invariance set. Otherwise, this group will be added to the invariance set. The second step is repeated until a stabilized invariance set has been found. Groups that were not assigned to the invariance set are considered to be non-invariant in that parameter. Alignment  is  one  of  the  best  studied  procedures  of  detecting  non-invariant  parameters  in  the context of studies with large numbers of groups. Asparouhov and MuthÃ©n (2014) conducted a series of simulation studies to evaluate the quality of the alignment method analyzing scenarios with five items with a number of groups set at 2, 3, 15, and 60, and group size set at 100 or 1,000. Finch (2016) conducted a simulation study to compare the alignment method with the Generalized Mantel-Haenszel  method,  generalized  logistic  regression,  and  the  Lordâ€™s  chi-square  test  on detecting uniform DIF across multiple-groups. Twenty dichotomous items with 2, 3, and 6 groups were employed. Lin (2020) conducted a study with 20 items in 24, 40 and 80 groups with sample sizes of 100 and 500. The picture is consistent: simulation studies showed that the detection rate was very high for thresholds at least as good as for well established methods for two groups. The detection of non-invariance of loadings was substantially worse than thresholds but acceptable in situations where the means of the groups were equal. Interestingly, in conditions with unequal means alignment methods would lead to incorrect DIF detection (Lin 2020). This, not a widely commented  fact,  is  especially  problematic  for  cross-country  comparisons  where  cross  country differences are very plausible. Logistic regression approach. In practice there are no established algorithms to detect non-invariance in multigroup settings for observed scores methods. However, as most of them showed good performance in comparisons of two groups sequential algorithms based on multiple comparisons could be proposed. As we have not found any established algorithms we propose an ad hoc procedure for this paper. We decided to use logistic regression for detecting non-invariance as proposed by Swaminathan and Rogers (1990) because of good performance of this method in two group settings and straightforward implementation. In the proposed approach, responses to the items are set as dependent variables. To the baseline model sum of item responses reflecting measured construct are added as independent variables: $+3#"@	=ğ›¼+ğ›½$ğ‘†"# ğ‘™ğ‘›	? 3#"where: ğœ‹"#	is a probability of a correct response for person p in group g, ğ‘†"# is a score, sum of item responses as a proxy for latent variable, ğ›½!	 identifies the relation between correct response and observed sum score,     (4) DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 8 In the model used to detect uniform DIF binary group indicator is added as second independent variable: 	ğ‘™ğ‘›	? 3#"$+3#"@	=ğ›¼+ğ›½$ğ‘†"#+ğ›½/ğº#   where: ğº#  is a binary indicator of a group membership, ğ›½/	 identifies uniform DIF effect,   (5)   For detecting uniform DIF log-likelihood ratio test is used comparing the baseline model with the model with the group indicator described in equation (5). For detecting non-uniform DIF the model is extended with interaction term:  ğ‘™ğ‘›	? 3#"$+3#"@	=ğ›¼+ğ›½$ğ‘†"#+ğ›½/ğº#+ğ›½4(ğ‘†"#âˆ—ğº#)  (6) where: (ğ‘†"#âˆ—ğº#) is an interaction variable,  ğ›½4	 identifies non-uniform DIF effect. Likelihood ratio test that compares model (5) with (6) is used to identify non-uniform DIF. This approach allows only for pairwise comparisons. Each group has to be compared with all others and based on those multiple comparisons conclusions about comparability must be drawn. Here, we are proposing a simple algorithm. In the first step, pairwise DIF analysis between all groups separately for all items is performed. For each comparison likelihood-ratio test with Benjamini-Hochberg adjustments for multiple comparisons (Benjamini & Hochberg, 1995) is performed. The p-value of log-likelihood ratio test is computed. For each item in each group, the number of DIF effects in pairwise comparisons against other groups is counted, where the DIF is considered as significant for p < 0.01. When the proportion of DIF effects in comparisons for an item in particular groups is larger than half of the groups, we flag that item as non-invariant. Uniform and non-uniform DIF effects are computed in separate comparisons. As mentioned above this procedure was invented ad hoc because of lack of established alternatives. However, the initial tests as well as  final  results  showed  that  the  procedure  is  very  effective  and  can  compete  with  more sophisticated approaches described earlier.  1.3 Proposed Approach The  proposed  approach  is  similar  to  the  idea  proposed  by  Zhang  and  colleagues  (2017).  This procedure consists of four steps. The first one involves examining the structure and features of the target data, that is the dataset where measurement invariance needs to be tested. In the second step, Monte Carlo simulations are used to generate data reflecting measurement situation and potential ranges of non-invariance. Thirdly, the generated data, that is sets of responses for all items and all groups, are used to train DNN. Finally, in the fourth step trained DNNs are used to predict item non-invariance in the target dataset.   DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 9 Step 1 Initialization  The  proposed  approach  starts  with  investigation  of  the  target  dataset.  This  is  done  to establish the dimensions of data structures (number of items, number of respondents and groups) and basic boundary conditions for Monte Carlo data generation in Step 2. For the measurement invariance the crucial element is the distribution of item and group parameters. To approximate true item parameters distributions, configural models could be run (items parameters estimated free in each group) and scalar model (assuming non-invariance) for obtaining the distributions for means  and  standard  deviations.  Of  course,  the  estimated  parameters  would  be  biased  in  the presence of non-invariance, however they would give reasonable boundaries for Monte Carlo data generation where simulated item parameters would be drawn for each iteration. In this step the size or sizes of misspecifications to be detected need to be determined. The good tool should detect non-invariance in parameters that are substantially important and not driven by random processes. This values could be based on previous simulation studies (eg. Pokropek, Davidov & Schmidt 2019; Pokropek, Schmidt & Davidov 2020), sensitivity analysis (Kuha & Moustaki 2015) or other methods like effect size indices for measurement invariance (Nye & Drasgow, 2011, Gomer, Jiang & Yuan 2019).  Step 2 Monte Carlo data generation The Monte Carlo data generation step consists of three phases. In the first one, a multigroup measurement  model  is  defined  and  distributions  of  parameters  are  provided  together  with distributions for size and allocation of non-invariance based on the outcomes of step 1. For the next  phase,  item  parameters  (item  thresholds  and  loadings)  and  non-invariance  values  (local misspecifications  for  thresholds  and  loadings  for  each  group)  are  sampled  from  distributions defined in the first phase. In the third, and final, phase,one dataset is generated containing item responses and group membership (based on sampled parameters). Together with the data matrix, a vector containing binary indicators for item bias is produced. The vector is constructed in such a way that it refers to each parameter in each group and takes a value of 0 for invariant parameters and 1 for non-invariant parameters. Once the data matrix and bias indicator vector is generated and  stored,  we  head  back  to  phase  two  and  continue  until  sufficient  numbers  of  data  sets  are generated. Empirical checks showed that for the applications presented in this paper 300 000 of datasets  and  invariance  vector  indicators  are  sufficient  for  good  detection  of  non-invariance parameters.  Step 3 Deep Neural Networks Learning  In the third step generated datasets are used to train DNN. Artificial neural network is a computing system consisting of (artificial) neurons, formed in layers and connected between them. The first layer is called the input layer, as it receives the features. Intuitively, the last one is called the output layer, which corresponds to prediction of the model. The layers between these two are referred to as the hidden layers, and when the neural network consists of more than one of them, it is classified as a deep neural network (DNN). The neurons from one layer are connected to other neurons from adjacent layers, and each connection has a weight assigned to it, representing its relative  importance.  The  number  of  such  weighted  connections  may  vary  depending  on  the architecture of the DNN. Each neuron has its internal state, called activation - when the n-th neuron in the layer is active, it generates an output zn, which then traverses by its output connections to the next layer and might be defined as:  DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 10 		ğ‘§5	=â„(âˆ‘6!72(ğ‘¤!âˆ—ğ‘¥!))	+	ğ‘)	           (7)  Where h denotes the activation function, N the number of neurons in the previous layer (or number of inputs to the model), x and w being i-th input and weight consecutively, and b being the optional bias (in our study, b=0 for all models in all layers). Then, the vector [z1, z2, â€¦, zn] becomes the input for the next layer or represents the predictions (if the layer is the output one). The graphical representation of equation (7) is depicted in Figure 2.    Figure 2. Graphical representation of the neuron in DNN and its relation input and output elements of the neural network  Currently, one of the most popular activation functions is the rectified linear unit (ReLU(z) = max(0, z)), due to its processing speed, and relative resistance to large input values. Contrary to, for example, logistic function or hyperbolic tangent, ReLU has no maximal value which helps to reduce issues during DNN estimation (Geron, 2019). The  process  of  calculating  weights  of  connections  between  neurons,  which  are  commonly initialized randomly, is called training. As in all of the machine learning algorithms, this task consists of a numerical optimization, i.e. a minimization of the loss function, often referred to as cost function, or in statistics, a likelihood function. The loss function is a metric that defines to what  extent  the  prediction  differs  from  expected  values;  for  example,  a  simple,  yet  powerful statistical metric such as the mean squared error is often used for this purpose. Loss function is also used in the weight optimization technique called backpropagation (Rumelhart et al., 1986) - the error at the NNâ€™s output is measured relative to the influence of the neurons of the previous layer, and this step is repeated until reaching the input layer. This influence is calculated using gradient of the loss function, associating it with a given state of the neuron, and finally the weights are updated to minimize the loss function. Most popular optimizers used for modern day NNs are stochastic gradient descent (SGD) with its improvements (momentum, Polyak, 1964; Nesterov, 1983),  as  well  as  adaptive  moment  estimation  (Adam,  Kingma  &  Ba,  2017),  or  adaptive subgradient (AdaGrad, Duchi et al., n.d.). Prediction in DNN may take different forms depending on the problem. For example, having a binary classifier (predicting a 0 or 1 outcome), we could rely on simply activation of the output neuron (active - non active), or on value of its output, i.e. magnitude of its activation function. DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 11 Elements that affect the training process, such as number of layers, number of neurons per layer, and parameters of the optimizer are called hyperparameters, and optimizing them to match the best performance of the NN is called tuning. For the whole process of modelâ€™s training and evaluation, the data has to be split into:  (1) the training set, used for learning purposes of the model,  (2) the test set, which is the data which have not been present to the model during  the training process to later evaluate it on, and eventually the  (3) validation set; popular mostly in DNNs (used to monitor the modelâ€™s performance during the training process).  This division, often referred to as train-test split, or train-test-validation split, usually implies the majority of the samples to be put into the training set (about 60 to 80% of the whole data). Although one may decide not to select any data for validation and focus on the metrics obtained from the trained model on the test set, having a validation set is useful for techniques such as early stopping, when the training process is automatically stopped after meeting some criteria, such as drastic increase in the loss function for validation data. Such an event may signalize, that the model is too optimized to fit the training data.  This phenomenon is called overfitting, and it is a huge issue of training complex machine learning models, such as DNNs. It is a good practise to fight overfitting by manipulating the architecture of the model - for DNNs, the most widely used techniques to prevent such phenomenon are dropout (Hinton et al., 2012; Srivastava et al., 2014) and batch normalization (Ioffe & Szegedy, 2015). Dropout works by randomly selecting a fraction of weights on the layer that should be ignored during  the  training  phase,  making  them  not  considered  during  any  pass  of  the  training  data throughout the DNN (this does not have an effect when making predictions). This yields in some of the neurons being â€˜inactiveâ€™ during a particular training phase, so they are effectively skipped. This procedure reduces the capacity of the DNN by introducing noise to the training, as with each pass of the data the number of active connections between the neurons vary. This may be compared to creating many layers in the DNN model and then producing a final one by averaging the result. As the data sets for deep learning are usually large they are usually split into batches - parts that are used for learning, i.e. optimization of weights of DNN. The batch might be referred to as a fraction of the whole training data. Furthermore, a process of the batch passing through the DNN forwards and backwards is called an epoch. Batch normalization for a given layer subtracts the mean from its output and divides the result by outputâ€™s standard deviation. It has been proven that batch normalization vastly speeds up the training process (Ioffe & Szegedy, 2015). For  the  proposed  application,  two  DNN  models  were  trained  based  on  generated  step  2  item responses and invariance indicator vector: one for non-invariant thresholds, and second for non-invariant loadings. Given that neural networks directly benefit from larger datasets (Alom et al., 2019), DNNs architecture that were tested for the proposed application consists of 8000 neurons in the first layer (exposed to various numbers of inputs, in our case - item responses), two hidden layers  with  8000  neurons  each,  and  the  output  layer.  Due  to  the  measurement  non-invariance detection  problem  being  rather  difficult  to  solve  using  linearly  separable  functions  alone,  we decided not to rely on â€˜shallowâ€™ ANN models and utilize the deep learning approach (Reed & Marks, 1999). Given the complexity of this problem and a relatively large number of inputs, the DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 12 model had to consist of many neurons in the hidden layers to provide a proper generalization (Goodfellow et al., 2016). Although there is no general rule nor analytical method for providing the most optimal values of the aforementioned hyper parameters (Goodfellow et al., 2016), we experimented with various promising architectures following the previously stated assumptions. The  presented  architecture  has  been  found  as  the  most  effective  one  for  our  study  given  its performance, as we conducted an effectiveness analysis based on the validation set, tuning the hyper parameters when necessary (which is a typical conduct for DNNs). For all layers except the output one, ReLU activation has been used, whereas the neurons responsible for prediction used sigmoid  function  (producing  the  probability  of  activation,  which  was  necessary  for  multilabel binary  classification  tasks  we  faced).  Batch  normalization  was  used  for  the  first  layer,  before dropouts not to introduce variance shift (X. Li et al., 2018). Both of the hidden layers are influenced by 50% dropout, reducing the complexity of the DNN during the training without oversimplifying the model. The graphical representation of used architectures can be seen in Figure 3. Please note that for all the presented studies we used the same DNN architecture, except the shape of the input and number of neurons in the output layer. This is also our recommendation for further studies.   Figure 3. Graphical representation of the architecture of used DNNs  DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 13  ğ¿(ğ‘¦L!,ğ‘¦!)=	âˆ’	$6âˆ‘6!7$[ğ‘¦!ğ‘™ğ‘œğ‘”(ğ‘¦L!)	+	(1	âˆ’	ğ‘¦!)âˆ—ğ‘¦L!	)	]  (8) Nesterov momentum optimization (Nesterov, 1983) has been used to optimize the loss function, for our case binary cross-entropy (also referred to as logloss function), given with the following formula: Where N is the number of predictions, ğ‘¦! is the i-th true value, and ğ‘¦L! the i-th predicted value. Learning rate was equal to 0.1 and momentum to 0.8. This high learning rate was proven effective for our study due to the complexity of the input data and due to the use of batch normalization, yielding improved generalization (Lewkowycz et al., 2020) as expected. With batch size equal to 256, approximately only 3 epochs were sufficient to train the model predicting the thresholds, and 8 to obtain satisfying metrics for the loadings, concerning all the studies. Step 4 Deep Neural Networks Predicting and Non-Invariance Flagging Once the parameters of DNN are estimated, the model is able to predict the outcomes from new data. Based on the trained models and input data for each item received, the probability of violation of conditional independence is computed. For each of the simulations, the input to the DNN is a one dimensional vector containing all item responses from a sample (or all) of respondents. Passing through the model, the output layer represents a one dimensional vector of probabilities (achieved by using sigmoid activation) of presence of either non-invariant thresholds or non-invariant factor loadings.  In different situations different probability thresholds could be used. We are proposing to establish cut-offs based on investigation of ROC (Receiver Operating Characteristic) curves to determine optimal  balance  between  true-positive  and  false-positive  rates  of  the  detection  in  particular situations. If no strong objectives for preferring greater rates of true-positive and false-positive rates optimal thresholds could be used. In the presented applications we used a method proposed by Liu (2012) to estimate the optimal cutpoint that maximises the product of the sensitivity and specificity.  If the approach would be successfully adopted by the research community and various DNNs would be trained and made available, an applied researcher could only perform the last step using trained networks produced by the specialists in the field of machine learning and statistics by choosing the one with the best documented performance and mostly suited structure for selected problems. Generally this would not be a big difference from the situation where applied researchers are using statistical software written by well trained specialists from statistics.  2 Simulation Study In this section we would like to show how the proposed approach behaves compared to traditional methods. We designed this study to reflect the real life situation of analysis with many groups, small  number  of  items  with  substantial  variation  between  latent  means  and  different  sizes  of parameters non-invariance. The conditions seem to be natural but surprisingly literature review shows that developed methods for handling many groups do not perform optimally in such settings. We  expect  that  our  method  would  outperform  previous  approaches  in  all  conditions  but DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 14 discrepancies will be largest for conditions with largest number of groups and small numbers of items in scale that is condition 4 with 10 groups and 3 items per scale. In order to do it, a simulation study has been conducted with its architecture as provided on Table 1.  Table 1 Simulation architecture for four conditions  Condition 1 Condition 2 Condition 3 Condition 4 Groups no. Observations per group Indicators no. 4 400 5 10 400 5  4 400 3 10 400 3   We based our simulations on empirical ESS data. We performed the analysis advised in step 1 of the new approach and investigated the empirical distributions of item parameters from the political participation scale of the 7th round of ESS (ESS 2014). We ran the MG-CFA configural model using  five  binary  items  designed  for  measuring  political  participation  and  investigated  the distribution of item parameters. The distribution of parameters from ESS could be approximated by skewed normal distribution with a mean of 2.95, a standard deviation of 1.22 and a skewness parameter of 10 for loadings and characterised by a skewed normal distribution with a mean of -0.21, a standard deviation of 10.64 and a skewness parameter of 0.72 for thresholds. To fit better to empirical results we additionally truncated the distributions of factor loadings to range between 1 and 7 and thresholds to range between -2 and 1. A Scalar multiple-group CFA (MG-CFA) model was used for establishing distributions of latent means. The good approximation of obtained group means was a normal distribution with zero mean and standard deviation equal to 0.2 and good approximation  of  within-group  standard  deviations  was  provided  by  a  uniform  distribution bounded by 0.75 and 1.25. Having established the possible distribution of parameters we generated the data.  In each single replication unique sets of item parameters, group means, and standard deviations were sampled from the distributions described above. Then data were generated for each group assuming measurement invariance i.e. the same item parameters for each group but different means and standard deviations of the construct. Next, randomly sampled parameters were modified to reflect violation of the invariance assumption.  Each of these parameters was changed by adding or subtracting a value that was sampled from a uniform distribution bounded by 0.3 and 1.0 for factor thresholds and 0.3 to 2.0 for factor loadings. Each parameter was randomly assigned to have a  positive  or  negative  sign  of  a  modification.  The  lower  bound  of  the  modification  describes medium non-invariance that moderately changes the results while the upper bound represents a very large magnitude of non-invariance. Those values are consistent with previous literature and simulation studies on measurement invariance and DIF (Kim, Yoon & Lee 2012, Woods 2009, Stark, Chernyshenko, & Drasgow, 2006, Flake & McCoach 2018). In each of the simulation datasets different values of item parameters and item non-invariances were  sampled  from  the  defined  distributions.  In  each  replication  the  number  of  non-invariant DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 15 groups was sampled (with restriction that at least one group should be non-invariant). Also, the number of non-invariant items in each group was randomly assigned for each replication with a minimum number of 1 and maximum of 3 (in 5 items conditions) and maximum of 2 (in 3 items conditions). For each simulation 5000 data sets were generated (the code for data generation is available in online supplementary material). On each dataset five methods of invariance detecting were performed: (1) modification indices (MI), (2) expected parameter change (EPC) - both of the indexes were obtained from a CFA/IRT scalar model, (3) logistic regression approach (Logistic), (4) alignment sequential DIF algorithm (alignment), and (5) DNN approach (DNN). For the DNN we generated separate additional 300 000 data sets for training the model. To obtain estimates for methods (1), (2), and (4) Mplus version 8 computer program was used (L. K. MuthÃ©n & MuthÃ©n, 2017). For (3) we implemented a custom algorithm using the difLogistic function from difR R package (Magis et al., 2010). For (5) we used Kerasâ€™ sequential model, provided from TensorFlow 2 package for Python3 (Abadi et al., 2015).  2.1 Simulation Study Results We present the results using ROC curves for MI, EPC indices and predicted probabilities of DNN. As approaches based on logistic regression and alignment optimization give only binary indicators (flagged and not flagged), only a single point for each of the methods is present on the plots. We show results separately for thresholds (left panel) and loadings (right panel). For each continuous measure of fit (MI, EPC and predicted probability from DNN) we present also the optimal cut-points (as proposed by Liu 2009).  ROC curves were computed on pulled data from all replications for each simulation condition investigating therefore the average detection rates among various generated data. For each of the ROC curves, the y-axis corresponds to the true-positive rate (TPR, also called sensitivity), that is the ratio of correctly classified positive samples (true positives (TP)), to sum of true positives and false negatives (FN, also called type II error). On the x-axis, the false-positive rate (FPR, also called false alarm ratio) might be observed, that is the ratio of false classifications of the positive class  (false  positives  (FP),  known  also  as  type  I  error)  and  a  sum  of  false  positives  and  true negatives (TN). Those two parameters are useful when describing performance of classification and detection algorithms, presenting how much of positive classes might be detected with making as small a number of mistakes as possible. If one would use the model which makes at most 1 false alarm (false positive) in 100 predictions, a value of TPR (y-axis) must be read where FPR = 0.01 (x-axis). The TPR here corresponds to the probability of detecting item non-invariance given that it is present, whereas FPR is the probability of detecting it while not appearing, i.e. making a false alarm. Furthermore, another useful evaluation metric which comes in handy in pair with the ROC curve is the AUC (area under curve) score, also referred to as AUROC in this application. AUROC takes values from 0 to 1, where 0 corresponds to the worst and 1 to the best possible performance of the model. It is calculated by simply computing the area between the ROC curve and x-axis (FPR). Although the standalone value might not be informative enough, together with ROC they provide a good approximation of the modelâ€™s performance. Condition 1: 4 groups 5 items Figure 4 presents results of the simulation study 1. In this scenario our approach has been proven to  be  the  most  effective  out  of  all  the  methods.  DNN  outperforms  other  methods  both  when detecting DIF in thresholds and factor loadings. For all techniques, the thresholds are much easier DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 16 to detect than factor loadings, and this will follow for all of the studies. Simple procedure based on a logistic model gives high TPR with low FPR for thresholds and yields similar results to DNN for factor loadings. Method based on alignment provides very low FPR with small TPR - this was expected as the method was designed to detect only the largest DIF. MI and EPC yield similar results with a slight advantage for MI.             (a) Thresholds               (b) Loadings  Figure 4. ROC curves for simulation study 1 (4 groups and 5 items)  Condition 2: 10 groups 5 items In this simulation study DNN has the best overall performance as well - it performs much better than MI and EPC, very similarly to the method based on logistic regression (on fixed level of TPR of 0.7). For factor loadings, the performance of DNN is similar to MI, being slightly better for high TPR values, but slightly worse when it comes to lower rates. Similar to study 1, method based on alignment provides very low TPR values.              DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 17     (a) Thresholds                (b) Loadings Figure 5. ROC curves for simulation study 2 (10 groups and 5 items) Condition 3: 4 groups 3 items In  settings  with  3  items  DNN  once  again  shows  its  superiority  over  existing  methods,  being substantially better than all other approaches both for detection of non-invariant thresholds and non-invariant loadings. The overall detection capability of the latter is quite poor for all methods, yet still with a notable advantage of DNN.             (a) Thresholds                              (b) Loadings Figure 6. ROC curves for simulation study 3 (4 groups and 3 items) Condition 4: 10 groups 3 items   DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 18 The final simulation results bring no surprise. Similarly to results from previous simulations, DNN performs best with the method based on logistic regression being very close (for one fixed value) for thresholds, but substantially worse for factor loadings. Detection based on MI and EPC yields mediocre performance, while detection based on the algorithm used in the alignment approach performs the worst.            (a) Thresholds                              (b) Loadings Figure 7. ROC curves for simulation study 4 (10 groups and 3 items)   2.2 Summary of the simulation study In Table 2 overall comparisons of the performance of the methods are presented by magnitudes of AUROC scores, where the best scores are bolded. Methods that used continuous indicators of DIF could not be directly compared with the methods that provide dichotomous flag (alignment and logistic) because the latter ones are simplified by only one empirical point, and they are not present in the table for this reason. Table  2  Results  of  simulation  studies.  Area  Under  ROC  (AUROC)  score,  excluding  methods providing dichotomous flag Condition 1 Loadings Thresholds 0.78 0.63 0.56 0.71 0.91 0.67 Condition 2 Loadings Thresholds 0.61 0.65 0.60 0.61 0.76 0.70 Condition 3 Loadings Thresholds 0.68 0.56 0.53 0.64 0.89   0.65 Condition 4 Thresholds Loading 0.58 0.65 0.60 0.57 0.76 0.70 Method MI EPC DNN  Note. Best performance bolded.  The  AUROC  comparisons,  remembering  its  shortcomings,  confirms  the  validity  of  DNN  in simulation settings as it yields the best scores for all studies in terms of this scoring. It needs to be noted however, that alignment and logistic methods were performing similarly to DNN in studies 2 and 4, and there are no AUROC scores for them.  DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 19 The  values  of  AUC  together  with  graphical  representation  of  ROC  curves  confirms  that  the proposed method based on DNN performs better than classical methods and in the majority of scenarios substantively outperforms the earlier methods. Our approach provides better detection rates  of  non-invariance  parameters  and  lower  false  detection  rates  than  previously  proposed methods for detecting non-invariance parameters in the situation of many groups. The proposed method is the only one that performs highly accurately for both thresholds and factor loadings. Especially for thresholds it provides significant improvement over the tested methods both for 5- and 3-items scales.  3 Empirical applications To show how the proposed methods could be applied to real-life data we are presenting 4 examples of applications related to four simulation studies presented earlier, based on ESS data. First two empirical examples are based on the 7th round of ESS (ESS 2014). For those we used five items designed  for  measuring  political  participation,  where  respondents  were  asked  whether  they: Contacted politician or government official last 12 months (i1), Worked in political party or action group last 12 months (i2), Worked in another organisation or association last 12 months (i3), Signed petition last 12 months (i4), and Boycotted certain products last 12 months (i5). The items which  measure  political  participation  were  coded  into  dichotomous  variables,  with  values  1 â€œparticipationâ€ and 0 â€œno participationâ€. We used the seventh round because within it a translation error was committed in the Slovenian questionnaire and reported by the ESS consortium â€“ the phrase â€œWorked in another organisation or associationâ€ was translated as â€œWorked in another political organisation or associationâ€. The item was excluded from the international dataset but was kept in the country-level data. We reintroduced this Slovenian item for our analysis in purpose to use it as a validation tool. We assume that the good detection method of measurement invariance should flag this item as non-invariant. This is a very unique opportunity to test the method in natural experimental settings which happens rarely in methodological studies on such large-scale data.  For examples three and four we used the most recent ESS data from round 9 (ESS, 2019) utilizing three items measuring political justice: Political system in country ensures everyone fair chance to participate in politics (i1); Government in country takes into account the interests of all citizens (i2); Decisions in country politics are transparent (i3). If respondents agreed with the statement their response was coded as 1, if disagreed as 0. In this situation we do not have â€œnaturallyâ€ non-invariant items, however we would like to show the behaviour of the investigated methods also in those settings.   For each scale we performed analysis on four countries and ten countries mimicking the conditions in the simulation studies. Therefore we use one of the advantages of the proposed method that is transferability of previously trained networks. To fit our data to the previously trained model we slightly modified the dataset.  For each example we sampled 400 respondents for each country including only respondents without missing data for investigated items. As our simulation study was using previously trained network based on real ESS data the first three steps of the proposed approach: Initialization (Step 1), Monte Carlo data generation (Step 2), Deep Neural Networks Learning (Step 3) could be omitted  for application on real data we only needed to perform Deep Neural Networks Predicting and Non-Invariance Flagging (Step 4), which simply boils down to using pre-trained networks from simulations and applying them on real data. We feed networks DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 20 with full vectors of responses from ESS datasets and based on this complete information predict the probability of non-invariance of parameters. To flag the non-invariance items we used the same optimal (by Liuâ€™s 2009 definition) cut points as in the simulation study.  3.1 Results of empirical study Table 3 presents the results of study one (left panel) and two (right panel). Countries are ordered alphabetically. Items are numbers corresponding to the description provided in previous sections. The most interesting results concern item 3 for Slovenia (SI) which was miss-translated, and we are  expecting  that  good  DIF  methods  should  flag  this  item  as  non-invariant.  In  fact,  in  two scenarios (both with 4 and 10 groups) the threshold of this item was flagged both by DNN and method based on pairwise logistic comparisons. Additionally, in scenario 1, EPC flagged it as a threshold and in scenario 2 as a loading. Although this is not the definitive test of the presented method,  such  a  result  goes  with  expectations  and  increases  our  confidence  in  this  approach. Moreover, it seems that two methods with best performance in the simulations provide results similar to each other, which once again would be something that we would expect from valid techniques.  Alignment methods have not flagged single items in both studies using four and ten groups. This could be expected as simulation studies show a very low detection rate of this approach. MI and EPC on the other hand marked too many items, almost all in study 2, which shows its limited usefulness in real data applications. Both DNN and the alignment-based method flagged much fewer items and seem to have greater levels of agreement than any other pair of methods. Table 4 presents results using ESS round 9 and 3 items referring to political justice. Here we do not have any â€œnaturallyâ€ non-invariant items. Simulation study was predicting a high level of false positives which is most probably reflected with a large number of flagged items in all methods, excluding approach based on alignment, which in simulation studies show very low levels of both true and false positive rates. The overall agreement between the methods is much smaller, which goes along the simulation results that suggest overall low detection rate and significant portion of false positives for majority of the examined methods.                DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 21 Table 3. Empirical results from ESS 7 for four and ten groups using political participation items Country (ISCO) Item  MI  EPC  ALI  LOG  DNN Study 1 Study 2 EPC  ALI LOG Country (ISCO) CZ  CZ  CZ  CZ  CZ  DE  DE  DE  DE  DE  ES ES ES ES ES FI FI FI FI FI FR  FR  FR  FR  FR  GB GB GB GB GB HU HU HU HU HU NO NO NO NO NO PL PL PL PL PL SI SI SI SI SI Item  MI i 1 2   3  4  5 1 b  2 l 3  4 5  b 1 l 2 b 3  4 5  i 1 l 2 b 3 4 l l 5 i 1 i 2 i 3 4 i l 5 b 1 i 2 3 b i 4  5 l 1 2 i b 3 b 4 b 5  1 2 i i 3 i 4 i 5 1   2  3  4 i 5 1   2  3*  4 5 i                                 i  i    i  i   i  i      l  i  i    i i i i i l l l l  b b b b i i b b b b i i i i i i i b b i b b b b b b i i i i i i  l i i i l l i                                                   DNN                 i  i  i  i  i     i  i   i  i  i      i  i              i  i  i         i  i l       i l i   i  i i  i i  b   i    i       i i  i                                 HU HU HU HU HU NO NO NO NO NO PL PL PL PL PL SI SI SI SI SI                               1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3* 4 5                                b    l b l    b                                        i    b b b b b b b b b i   i  i                                                                                 b  i i    i l i i  i  i   i   Note: i- Threshold flagged as non-invariant; l â€“ loading flagged as non-invariant; b- both Threshold and loading flagged as non-invariant; * naturally non-invariant item DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 22 Table 4. Empirical results from ESS round 9 for four and ten groups using social justice items Country (ISCO) Item  MI  EPC  ALI  LOG  DNN Study 3 Study 4 EPC  ALI LOG DNN  Country (ISCO) CZ  CZ  CZ  DE  DE  DE  ES ES ES FI FI FI FR  FR  FR  GB GB GB HU HU HU NO NO NO PL PL PL SI SI SI Item  MI i 1 2   3 i 1  2 3   1  2  3 1   2  3 l 1 l 2 3 l  1 l 2  3 1 b i 2 b 3 i 1 i 2 3 i i 1 i 2 i 3 1 b b 2 3 b                   l l  b b  l  l  b  i         l l l l l  l l  b l b b l i b l  b l l                              i i  i   i             i  i i  b i  i i i i  l  i b l l  b l        l i  l b i  l    i l                   HU HU HU NO NO NO PL PL PL SI SI SI                   1 2 3 1 2 3 1 2 3 1 2 3                   b i l b  b b b b b b b                   i   i b b  l i i  l                    l                             i  i i  b i  b i i i Note i- Threshold flagged as non-invariant; l â€“ loading flagged as non-invariant; b- both Threshold and loading flagged as non-invariant   4 Discussion and Conclusions The novel approach of non-invariance detection based on DNN proposed in this paper has yielded very promising results. Simulation studies show that it performs substantially better in the majority of situations, providing greater balance between false positive and true positive rates, while in all other cases it performs no worse than classical approaches. We showed that it could be successfully applied  to  real-life  data  and  that  it  correctly  flags  items  that  were  previously  known  to  be (â€œnaturallyâ€) non-invariant. The performed simulations are not without limitations - as all simulation studies they are bound to a fixed set of conditions. We have tried to avoid it by carefully picking such sets that would reflect the real data and most plausible situations.  Of course we are open for forthcoming studies that would like to test our approach in different conditions.    Simulations are a really critical part of our approach.  In the presented study DNNs were trained on simulated data and then compared with other approaches using the same conditions that were DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 23 used  for  training  DNNs.  One  of  the  Reviewers  pointed  out  that  such  an  approach  is  a  little tautological - although agreeing with such reasoning, we do not see that this is a necessary problem. In  classical  approach  model  assumptions  are  stated  directly  by  model  formulation.  For  our approach the assumptions need to be delivered by learning data that were generated using Monte Carlo simulations using some assumptions on the nature of invariance. In this paper, therefore, we have proven that our approach works best if the set of conditions defined by simulation settings are fulfilled. The different question is whether this approach performs well if our assumptions about  the  measurement  invariance  will  be  violated  in  real  data  and  how  much  the  presented approach  is  robust  for  such  violations.  Similar  questions  could  be  also  asked  for  classical approaches. For example, what would happen if we would generate data from other models than CFA/IRT.  Those are legitimate questions but in our opinion out of the scope that could be provided in one article. This is certainly the direction for further studies.  From the other side, the approach could be criticized that for successful application we need to precisely design data for simulation, and the created model will be able to detect only specific types of misfit. As it could be treated as problematic, on the other hand this is a big advantage which allows us to specify in a very detailed way what kind of DIF or misspecification should be detected and what rates of error are acceptable for the users. In  our  application  we  showed  that  the  approach  based  on  DNNs  could  outperform  traditional methods. The results however still could be considered unsatisfactory by some (especially if we are looking at detection rates for factor loadings). In our application we used relatively shallow DNN architecture that could be handled by a desktop computer. Given the ongoing development of computational resources, we might expect improvements in utilizing DNNs for not only the study of this paper, but also for other problems regarding social sciences. Presented approach was applied on detection of DIF (or non-invariance) in the context of many groups and short scales and with plausible assumption that groups substantially differ in latent means amongst each other. This task was not optimally handled by classical approaches and our proposition is a substantial improvement in efficiency of detecting non-invariance in the described context.  The  presented  approach  could  also  be  applied  to  other  situations  where  model  miss-specification or local misspecification of statistical model needs to be detected. This could apply to a wide range of more complicated statistical models where detecting model misspecification or data  diagnostic  is  not  trivial  e.g.  dimensionality  analysis,  assessing,  detecting  model misspecification in complex non-linear models (e.g. inflated beta regression), detecting response styles or random-effects model misspecification in a rich class of mixed effects models.  Models and examples described in this paper could be handled easily by a single researcher on a desktop  computer.  However,  the  proposed  method  still  needs  some  high  level  knowledge  on statistical models (for simulation part) and machine learning algorithms (for learning part). Next steps of development of the approach based on DNN could involve generating a large number of different examples and provide end users with pre-trained networks with easy user interfaces. For a single researcher it might be hard to achieve, since more complex networks with a higher number of  datasets  to  learn  would  require  to  use  supercomputers  and  more  specialized  knowledge. However, in the context of institutions that are releasing data from many countries and would like to  assure  the  data  quality  and  comparability,  such  procedures  are  already  within  reach.  Once trained DNN could be reused in different situations, as it was shown in our empirical example. DEEP NEURAL NETWORKS AND MEASUREMENT INVARIANCE 24 DNNs prepared by experts could be further used by applied researchers as nowadays statistical packages or open-source software are widely used. There is no reason why tools based on DNN could not be implemented in standard statistical software. The fundamental change is how the statistical device would be developed. In the traditional way it is written using formal specified assumptions in proposed approaches learned by loads of artificial and (possibly) true data. Recent development of artificial intelligence based on DNN shows that the second path in some situations could be also effective - DNNs achieve the most spectacular successes in problems that are too complex for regular ANNs to solve, as for example the field of image recognition. The current DNN solutions achieve an accuracy of 98 percent in such tasks. In the recent years, deep learning has also made contributions not only to applied research; Baldi and colleagues (2014) demonstrate that DNN could be successfully used for analyzing subatomic particle collisions at high-energy particle colliders, improving the classification metrics by as much as 8% over the best approaches that are currently in use. DNNs are used in large physics laboratories including Large Hadron Collider at CERN for classification purposes and anomaly detections in particle data (Bhimji et al., 2018). Several works demonstrate that DNNs provide superior performance in predicting the behavior of molecules in pharmaceutical research (Dahl et al., 2014; Wallach et al., 2015). Another area where DNNs were successfully used is applied mathematics: a good example is the three-body problem, which concerns calculating the exact positions of bodies in the future. Although it is possible to solve using conventional techniques, it requires enormous computational resources. Recently, it was shown that DNNs could provide accurate solutions at a fixed computational cost and up to 100 million times faster than state-of-the-art conventional solvers (Breen et al., 2019). Similar approaches are used in astronomy to model galaxy prosperities (Ravanbakhsh et al., 2017) and  galaxy  classifications  (Kennamer  et  al.,  2018).  Also,  for  investigating  the  sequence specificities of DNA- and RNA-binding proteins (Alipanahi et al., 2015), or performing DNA sequence classification (Bosco & Di Gangi, 2016), DNN has shown its usefulness. Although an exhaustive review of DNN usage in basic research is beyond the scope of this paper, successful application of DNN in climate modeling (Rasp et al., 2018), material science (Mozaffar et al., 2019), ecology (Browning et al., 2018) and in neuroscience (Marblestone et al., 2016) should be mentioned. Why should it not work for developing better sociological methods and research?  